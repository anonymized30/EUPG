{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d3343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import Counter\n",
    "import csv\n",
    "import scipy\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from datasets import *\n",
    "from mdav import *\n",
    "from train import *\n",
    "from models import *\n",
    "from attacks import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1531e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "\n",
    "# Filter out ConvergenceWarning and FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Assuming y_test and y_forget are arrays of class indices\n",
    "encoder = OneHotEncoder(sparse_output=False, categories=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_everything(seed=7):\n",
    "#     np.random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# seed_everything(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba412321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, Count=89556, Percentage=93.08%\n",
      "Class=1, Count=6659, Percentage=6.92%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get dataset\n",
    "\n",
    "df=pd.read_csv('data/GiveMeSomeCredit/cs-training.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "y = df['SeriousDlqin2yrs'].values\n",
    "df.drop(df[['SeriousDlqin2yrs']],axis=1,inplace=True)\n",
    "X = df.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "SC = StandardScaler()\n",
    "X_train = SC.fit_transform(X_train)\n",
    "X_test = SC.transform(X_test)\n",
    "\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))\n",
    "    \n",
    "num_features = X_train.shape[-1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_model = XGBClassifier(num_classes= num_classes, reg_lambda=5, \n",
    "                              learning_rate=0.5, max_depth=9, n_estimators=200, device = device)\n",
    "n_repeat = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61c4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training M on D time:2.34(±0.00) seconds\n",
      "Train AUC:99.86(±0.00)%\n",
      "Test AUC:80.80(±0.00)%\n",
      "MIA AUC:55.72(±0.00)%\n",
      "MIA Advantage:8.52(±0.00)%\n",
      "Retraining M on D_ret time:3.68(±0.00) seconds\n",
      "Retain AUC:99.93(±0.00)%\n",
      "Forget AUC:79.73(±0.00)%\n",
      "Test AUC:80.24(±0.00)%\n",
      "MIA AUC:49.90(±0.00)%\n",
      "MIA Advantage:2.04(±0.00)%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████▍  | 92790/96215 [01:42<00:00, 12438.25it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 96215/96215 [01:43<00:00, 933.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 9621\n",
      "Mean of mean distances to centroids: 2.0463859581150317\n",
      "Shape of X_train_k:(96215, 10), y_train_k:(96215,)\n",
      "----------------------------------------\n",
      "k= 10 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:103.28(±0.00)\n",
      "Training M_k on D_k time:3.33(±0.00)\n",
      "Train AUC:90.44(±0.00)%\n",
      "Test AUC:82.29(±0.00)%\n",
      "MIA AUC:51.95(±0.00)%\n",
      "MIA Advantage:4.10(±0.00)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:0.51(±0.00)\n",
      "Train AUC:86.76(±0.00)%\n",
      "Test AUC:82.20(±0.00)%\n",
      "MIA AUC:50.42(±0.00)%\n",
      "MIA Advantage:1.79(±0.00)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:0.50(±0.00) seconds\n",
      "Retain AUC:86.91(±0.00)%\n",
      "Forget AUC:83.13(±0.00)%\n",
      "Test AUC:82.24(±0.00)%\n",
      "MIA AUC:51.21(±0.00)%\n",
      "MIA Advantage:2.29(±0.00)%\n",
      "----------------------------------------\n",
      "Training M on D time:2.94(±0.00) seconds\n",
      "Train AUC:99.86(±0.00)%\n",
      "Test AUC:80.80(±0.00)%\n",
      "MIA AUC:56.63(±0.00)%\n",
      "MIA Advantage:8.90(±0.00)%\n",
      "Retraining M on D_ret time:3.62(±0.00) seconds\n",
      "Retain AUC:99.97(±0.00)%\n",
      "Forget AUC:80.39(±0.00)%\n",
      "Test AUC:79.76(±0.00)%\n",
      "MIA AUC:50.41(±0.00)%\n",
      "MIA Advantage:1.22(±0.00)%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|██████████████████████████████████████████████████████████████████████▋  | 93090/96215 [01:42<00:00, 12584.03it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 96215/96215 [01:43<00:00, 930.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 9621\n",
      "Mean of mean distances to centroids: 2.0463859581150317\n",
      "Shape of X_train_k:(96215, 10), y_train_k:(96215,)\n",
      "----------------------------------------\n",
      "k= 10 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:103.61(±0.00)\n",
      "Training M_k on D_k time:3.98(±0.00)\n",
      "Train AUC:90.44(±0.00)%\n",
      "Test AUC:82.29(±0.00)%\n",
      "MIA AUC:50.37(±0.00)%\n",
      "MIA Advantage:1.28(±0.00)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:0.50(±0.00)\n",
      "Train AUC:86.76(±0.00)%\n",
      "Test AUC:82.20(±0.00)%\n",
      "MIA AUC:50.51(±0.00)%\n",
      "MIA Advantage:1.17(±0.00)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:0.45(±0.00) seconds\n",
      "Retain AUC:87.09(±0.00)%\n",
      "Forget AUC:83.82(±0.00)%\n",
      "Test AUC:81.97(±0.00)%\n",
      "MIA AUC:50.37(±0.00)%\n",
      "MIA Advantage:1.09(±0.00)%\n",
      "----------------------------------------\n",
      "Training M on D time:3.56(±0.00) seconds\n",
      "Train AUC:99.86(±0.00)%\n",
      "Test AUC:80.80(±0.00)%\n",
      "MIA AUC:62.24(±0.00)%\n",
      "MIA Advantage:21.70(±0.00)%\n",
      "Retraining M on D_ret time:3.25(±0.00) seconds\n",
      "Retain AUC:100.00(±0.00)%\n",
      "Forget AUC:79.81(±0.00)%\n",
      "Test AUC:79.55(±0.00)%\n",
      "MIA AUC:50.60(±0.00)%\n",
      "MIA Advantage:21.23(±0.00)%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████  | 93600/96215 [01:41<00:00, 13063.33it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 96215/96215 [01:42<00:00, 939.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 9621\n",
      "Mean of mean distances to centroids: 2.0463859581150317\n",
      "Shape of X_train_k:(96215, 10), y_train_k:(96215,)\n",
      "----------------------------------------\n",
      "k= 10 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:102.65(±0.00)\n",
      "Training M_k on D_k time:3.92(±0.00)\n",
      "Train AUC:90.44(±0.00)%\n",
      "Test AUC:82.29(±0.00)%\n",
      "MIA AUC:57.94(±0.00)%\n",
      "MIA Advantage:18.81(±0.00)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:0.50(±0.00)\n",
      "Train AUC:86.76(±0.00)%\n",
      "Test AUC:82.20(±0.00)%\n",
      "MIA AUC:60.77(±0.00)%\n",
      "MIA Advantage:22.97(±0.00)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:0.36(±0.00) seconds\n",
      "Retain AUC:87.90(±0.00)%\n",
      "Forget AUC:83.93(±0.00)%\n",
      "Test AUC:81.74(±0.00)%\n",
      "MIA AUC:55.61(±0.00)%\n",
      "MIA Advantage:16.35(±0.00)%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample retain and forget sets\n",
    "forget_ratios = [0.05, 0.2, 0.5]\n",
    "for forget_ratio in forget_ratios:\n",
    "    idxs = np.arange(len(y_train))\n",
    "    random.shuffle(idxs)\n",
    "    m = int(len(y_train)*forget_ratio)\n",
    "    retain_idxs = idxs[m:]\n",
    "    forget_idxs = idxs[:m]\n",
    "    X_retain = X_train[retain_idxs]\n",
    "    y_retain = y_train[retain_idxs]\n",
    "    X_forget = X_train[forget_idxs]\n",
    "    y_forget = y_train[forget_idxs]\n",
    "\n",
    "    # Step 2: Define and train M on D\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    mia_aucs = []\n",
    "    mia_advs = []\n",
    "    runtimes = []\n",
    "\n",
    "    for r in range(n_repeat):\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        t0 = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "        rt = t1-t0\n",
    "        runtimes.append(rt)\n",
    "\n",
    "        # Evaluate the model accuracy, and MIA\n",
    "        # Accuracy\n",
    "        train_acc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "        test_acc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        train_accs.append(100.0*train_acc)\n",
    "        test_accs.append(100.0*test_acc)\n",
    "        #MIA\n",
    "        idxs = np.arange(len(y_test))\n",
    "        random.shuffle(idxs)\n",
    "        rand_idxs = idxs[:m]\n",
    "\n",
    "        test_preds = model.predict_proba(X_test)\n",
    "        forget_preds = model.predict_proba(X_forget)\n",
    "\n",
    "        # Convert class indices to one-hot encoding\n",
    "        y_test_one_hot = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "        y_forget_one_hot = encoder.transform(y_forget.reshape(-1, 1))\n",
    "\n",
    "        loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "        loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "        attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                  loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                  train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "        auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "        adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "        mia_aucs.append(100.0*auc)\n",
    "        mia_advs.append(100.0*adv)\n",
    "\n",
    "    mean_runtime = np.mean(runtimes)\n",
    "    std_runtime = np.std(runtimes)\n",
    "    mean_train_acc = np.mean(train_accs)\n",
    "    std_train_acc = np.std(train_accs)\n",
    "    mean_test_acc = np.mean(test_accs)\n",
    "    std_test_acc = np.std(test_accs)\n",
    "    mean_mia_auc = np.mean(mia_aucs)\n",
    "    std_mia_auc = np.std(mia_aucs)\n",
    "    mean_mia_adv = np.mean(mia_advs)\n",
    "    std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "    # Print the results\n",
    "    print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "    print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "    print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "    print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "    print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_file_path = 'results/credit/xgb_m_d_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "        writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "        writer.writerow(['Train AUC', mean_train_acc, std_train_acc])\n",
    "        writer.writerow(['Test AUC', mean_test_acc, std_test_acc])\n",
    "        writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "        writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n",
    "\n",
    "    ######################################################################################################\n",
    "    # Step 3: Train M_retain on D_retain\n",
    "    retain_accs = []\n",
    "    forget_accs = []\n",
    "    test_accs = []\n",
    "    mia_aucs = []\n",
    "    mia_advs = []\n",
    "    runtimes = []\n",
    "    for r in range(n_repeat):\n",
    "        model_ret = copy.deepcopy(initial_model)\n",
    "        t0 = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "        model_ret.fit(X_retain, y_retain)\n",
    "        t1 = time.time()\n",
    "        rt = t1-t0\n",
    "        runtimes.append(rt)\n",
    "\n",
    "        # Evaluate the model accuracy, and MIA\n",
    "        # Accuracy\n",
    "        retain_acc = roc_auc_score(y_retain, model_ret.predict_proba(X_retain)[:, 1])\n",
    "        forget_acc = roc_auc_score(y_forget, model_ret.predict_proba(X_forget)[:, 1])\n",
    "        test_acc = roc_auc_score(y_test, model_ret.predict_proba(X_test)[:, 1])\n",
    "        retain_accs.append(100.0*retain_acc)\n",
    "        forget_accs.append(100.0*forget_acc)\n",
    "        test_accs.append(100.0*test_acc)\n",
    "        #MIA\n",
    "        idxs = np.arange(len(y_test))\n",
    "        random.shuffle(idxs)\n",
    "        rand_idxs = idxs[:m]\n",
    "\n",
    "        test_preds = model_ret.predict_proba(X_test)\n",
    "        forget_preds = model_ret.predict_proba(X_forget)\n",
    "        loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "        loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "        attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                  loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                  train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "        auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "        adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "        mia_aucs.append(100.0*auc)\n",
    "        mia_advs.append(100.0*adv)\n",
    "\n",
    "\n",
    "    mean_retrain_runtime = np.mean(runtimes)\n",
    "    std_retrain_runtime = np.std(runtimes)\n",
    "    mean_retain_acc = np.mean(retain_accs)\n",
    "    std_retain_acc = np.std(retain_accs)\n",
    "    mean_forget_acc = np.mean(forget_accs)\n",
    "    std_forget_acc = np.std(forget_accs)\n",
    "    mean_retrain_test_acc = np.mean(test_accs)\n",
    "    std_retrain_test_acc = np.std(test_accs)\n",
    "    mean_retrain_mia_auc = np.mean(mia_aucs)\n",
    "    std_retrain_mia_auc = np.std(mia_aucs)\n",
    "    mean_retrain_mia_adv = np.mean(mia_advs)\n",
    "    std_retrain_mia_adv = np.std(mia_advs)\n",
    "\n",
    "    # Print the results\n",
    "    print('Retraining M on D_ret time:{:0.2f}(±{:0.2f}) seconds'.format(mean_retrain_runtime, std_retrain_runtime))\n",
    "    print('Retain AUC:{:0.2f}(±{:0.2f})%'.format(mean_retain_acc, std_retain_acc))\n",
    "    print('Forget AUC:{:0.2f}(±{:0.2f})%'.format(mean_forget_acc, std_forget_acc))\n",
    "    print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_retrain_test_acc, std_retrain_test_acc))\n",
    "    print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_auc, std_retrain_mia_auc))\n",
    "    print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_adv, std_retrain_mia_adv))\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_retrain_file_path = 'results/credit/xgb_mret_dret_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "    with open(csv_retrain_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "        writer.writerow(['Retraining Time', mean_retrain_runtime, std_retrain_runtime])\n",
    "        writer.writerow(['Retain AUC', mean_retain_acc, std_retain_acc])\n",
    "        writer.writerow(['Forget AUC', mean_forget_acc, std_forget_acc])\n",
    "        writer.writerow(['Test AUC', mean_retrain_test_acc, std_retrain_test_acc])\n",
    "        writer.writerow(['MIA AUC', mean_retrain_mia_auc, std_retrain_mia_auc])\n",
    "        writer.writerow(['MIA Advantage', mean_retrain_mia_adv, std_retrain_mia_adv])\n",
    "        \n",
    "# ######################################################################################################\n",
    "    # Step 1: k-anonymize and prepare D_k\n",
    "    ft_epochs_list = [5]\n",
    "    for ft_epochs in ft_epochs_list:\n",
    "        K = [10]\n",
    "        for k in K:\n",
    "            runtimes_k = []\n",
    "            t0 = time.time()\n",
    "            centroids, clusters, labels, X_train_k, y_train_k = mdav(copy.deepcopy(X_train), copy.deepcopy(y_train), k)\n",
    "            print_stats(clusters, centroids)\n",
    "            print('Shape of X_train_k:{}, y_train_k:{}'.format(X_train_k.shape, y_train_k.shape))\n",
    "             # Create TensorDatasets\n",
    "            t1 = time.time()\n",
    "            rt_k = t1- t0\n",
    "            runtimes_k.append(rt_k)\n",
    "\n",
    "            train_accs_k = []\n",
    "            test_accs_k = []\n",
    "            mia_aucs_k = []\n",
    "            mia_advs_k = []\n",
    "            runtimes_train_k = []\n",
    "\n",
    "            train_accs_k_D = []\n",
    "            test_accs_k_D = []\n",
    "            mia_aucs_k_D = []\n",
    "            mia_advs_k_D = []\n",
    "            runtimes_train_k_D = []\n",
    "\n",
    "            retain_accs_k_ret = []\n",
    "            forget_accs_k_ret = []\n",
    "            test_accs_k_ret = []\n",
    "            mia_aucs_k_ret = []\n",
    "            mia_advs_k_ret = []\n",
    "            runtimes_train_k_ret = []\n",
    "\n",
    "            for r in range(n_repeat):\n",
    "                model_k = copy.deepcopy(initial_model)\n",
    "                t0 = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                model_k.fit(X_train_k, y_train_k)\n",
    "                t1 = time.time()\n",
    "                rt_train = t1- t0\n",
    "                runtimes_train_k.append(rt_train)\n",
    "\n",
    "                # Evaluate the model accuracy, and MIA\n",
    "                # Accuracy\n",
    "                train_acc = roc_auc_score(y_train_k, model_k.predict_proba(X_train_k)[:, 1])\n",
    "                test_acc = roc_auc_score(y_test, model_k.predict_proba(X_test)[:, 1])\n",
    "                train_accs_k.append(100.0*train_acc)\n",
    "                test_accs_k.append(100.0*test_acc)\n",
    "                #MIA\n",
    "                idxs = np.arange(len(y_test))\n",
    "                random.shuffle(idxs)\n",
    "                rand_idxs = idxs[:m]\n",
    "\n",
    "                test_preds = model_k.predict_proba(X_test)\n",
    "                forget_preds = model_k.predict_proba(X_forget)\n",
    "\n",
    "                # Convert class indices to one-hot encoding\n",
    "                y_test_one_hot = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "                y_forget_one_hot = encoder.transform(y_forget.reshape(-1, 1))\n",
    "\n",
    "                loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "                loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "                attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                          loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                          train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "                auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                mia_aucs_k.append(100.0*auc)\n",
    "                mia_advs_k.append(100.0*adv)\n",
    "\n",
    "                model_k_D = copy.deepcopy(initial_model)\n",
    "                model_k_D.set_params(learning_rate = 0.5, n_estimators=ft_epochs)\n",
    "                t0 = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                model_k_D.fit(X_train, y_train, xgb_model=model_k)\n",
    "                t1 = time.time()\n",
    "                rt = t1-t0\n",
    "                runtimes_train_k_D.append(rt)\n",
    "\n",
    "                # Evaluate the model accuracy, and MIA\n",
    "                # Accuracy\n",
    "                train_acc = roc_auc_score(y_train, model_k_D.predict_proba(X_train)[:, 1])\n",
    "                test_acc = roc_auc_score(y_test, model_k_D.predict_proba(X_test)[:, 1])\n",
    "                train_accs_k_D.append(100.0*train_acc)\n",
    "                test_accs_k_D.append(100.0*test_acc)\n",
    "                #MIA\n",
    "                idxs = np.arange(len(y_test))\n",
    "                random.shuffle(idxs)\n",
    "                rand_idxs = idxs[:m]\n",
    "\n",
    "                test_preds = model_k_D.predict_proba(X_test)\n",
    "                forget_preds = model_k_D.predict_proba(X_forget)\n",
    "\n",
    "                # Convert class indices to one-hot encoding\n",
    "                y_test_one_hot = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "                y_forget_one_hot = encoder.transform(y_forget.reshape(-1, 1))\n",
    "\n",
    "                loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "                loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "                attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                          loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                          train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "                auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                mia_aucs_k_D.append(100.0*auc)\n",
    "                mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "                model_k_ret = copy.deepcopy(initial_model)\n",
    "                model_k_ret.set_params(learning_rate = 0.5, n_estimators=ft_epochs)\n",
    "                t0 = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                model_k_ret.fit(X_retain, y_retain, xgb_model=model_k)\n",
    "                t1 = time.time()\n",
    "                rt = t1-t0\n",
    "                runtimes_train_k_ret.append(rt)\n",
    "                # Evaluate the model accuracy, and MIA\n",
    "                # Accuracy\n",
    "                retain_acc = roc_auc_score(y_retain, model_k_ret.predict_proba(X_retain)[:, 1])\n",
    "                forget_acc = roc_auc_score(y_forget, model_k_ret.predict_proba(X_forget)[:, 1])\n",
    "                test_acc = roc_auc_score(y_test, model_k_ret.predict_proba(X_test)[:, 1])\n",
    "                retain_accs_k_ret.append(100.0*retain_acc)\n",
    "                forget_accs_k_ret.append(100.0*forget_acc)\n",
    "                test_accs_k_ret.append(100.0*test_acc)\n",
    "                #MIA\n",
    "                idxs = np.arange(len(y_test))\n",
    "                random.shuffle(idxs)\n",
    "                rand_idxs = idxs[:m]\n",
    "\n",
    "                test_preds = model_k_ret.predict_proba(X_test)\n",
    "                forget_preds = model_k_ret.predict_proba(X_forget)\n",
    "                loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "                loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "                attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                          loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                          train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "                auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                mia_aucs_k_ret.append(100.0*auc)\n",
    "                mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "            # Anonymizing D and training M_k on D_k\n",
    "            mean_anonymize_time = np.mean(runtimes_k)\n",
    "            std_anonymize_time = np.std(runtimes_k)\n",
    "            mean_train_k_time = np.mean(runtimes_train_k)\n",
    "            std_train_k_time = np.std(runtimes_train_k)\n",
    "            mean_train_k_acc = np.mean(train_accs_k)\n",
    "            std_train_k_acc = np.std(train_accs_k)\n",
    "            mean_test_k_acc = np.mean(test_accs_k)\n",
    "            std_test_k_acc = np.std(test_accs_k)\n",
    "            mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "            std_mia_k_auc = np.std(mia_aucs_k)\n",
    "            mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "            std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "            # Finetuning M_k on D\n",
    "            mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "            std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "            mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "            std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "            mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "            std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "            mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "            std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "            mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "            std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "            # Finetuning M_k on D_ret\n",
    "            mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "            std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "            mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "            std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "            mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "            std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "            mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "            std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "            mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "            std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "            mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "            std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "\n",
    "            # Print the results\n",
    "            print('----------------------------------------')\n",
    "            print('k=', k, 'Fine-tuning epochs=', ft_epochs)\n",
    "            print('----------------------------------------')\n",
    "            print('-----Anonymizing D and training M_k on D_k-----')\n",
    "            print('Anonymizing D time:{:0.2f}(±{:0.2f})'.format(mean_anonymize_time, std_anonymize_time))\n",
    "            print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "            print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "            print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "            print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "            print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "            print('-----Finetuning M_k on D-----')\n",
    "            print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "            print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "            print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "            print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "            print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "            print('-----Finetuning M_k on D_ret-----')\n",
    "            print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "            print('Retain AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "            print('Forget AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "            print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "            print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "            print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "            print('----------------------------------------')\n",
    "\n",
    "            # Save to CSV\n",
    "            csv_anonymize_file_path = 'results/credit/xgb_mk={}_dk_fr={}.csv'.format(k, forget_ratio)\n",
    "            csv_finetune_D_file_path = 'results/credit/xgb_mk={}_d_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "            csv_finetune_D_ret_file_path = 'results/credit/xgb_mk={}_dret_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "\n",
    "            # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "            with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                writer.writerow(['Anonymizing Time', mean_anonymize_time, std_anonymize_time])\n",
    "                writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "                writer.writerow(['Train AUC', mean_train_k_acc, std_train_k_acc])\n",
    "                writer.writerow(['Test AUC', mean_test_k_acc, std_test_k_acc])\n",
    "                writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "                writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "            with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "                writer.writerow(['Train AUC', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "                writer.writerow(['Test AUC', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "                writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "                writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "            with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "                writer.writerow(['Retain AUC', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "                writer.writerow(['Forget AUC', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "                writer.writerow(['Test AUC', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "                writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "                writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac39efc",
   "metadata": {},
   "source": [
    "# Differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 5 0.5 (120269, 10) (120269,)\n",
      "----------------------------------------\n",
      "Epsilon= 0.5 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Training M_k on D_k time:2.76(±0.00)\n",
      "Train accuracy:70.98(±0.00)%\n",
      "Test accuracy:58.33(±0.00)%\n",
      "MIA AUC:50.78(±0.00)%\n",
      "MIA Advantage:2.47(±0.00)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:0.32(±0.00)\n",
      "Train accuracy:57.72(±0.00)%\n",
      "Test accuracy:56.84(±0.00)%\n",
      "MIA AUC:50.18(±0.00)%\n",
      "MIA Advantage:0.81(±0.00)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:0.32(±0.00) seconds\n",
      "Retain accuracy:61.90(±0.00)%\n",
      "Forget accuracy:60.92(±0.00)%\n",
      "Test accuracy:60.71(±0.00)%\n",
      "MIA AUC:49.97(±0.00)%\n",
      "MIA Advantage:0.48(±0.00)%\n",
      "----------------------------------------\n",
      "0.2 5 0.5 (120269, 10) (120269,)\n",
      "----------------------------------------\n",
      "Epsilon= 0.5 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Training M_k on D_k time:2.82(±0.00)\n",
      "Train accuracy:70.98(±0.00)%\n",
      "Test accuracy:58.33(±0.00)%\n",
      "MIA AUC:50.20(±0.00)%\n",
      "MIA Advantage:1.09(±0.00)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:0.32(±0.00)\n",
      "Train accuracy:57.72(±0.00)%\n",
      "Test accuracy:56.84(±0.00)%\n",
      "MIA AUC:50.14(±0.00)%\n",
      "MIA Advantage:0.28(±0.00)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:0.26(±0.00) seconds\n",
      "Retain accuracy:57.78(±0.00)%\n",
      "Forget accuracy:57.45(±0.00)%\n",
      "Test accuracy:56.84(±0.00)%\n",
      "MIA AUC:50.13(±0.00)%\n",
      "MIA Advantage:0.82(±0.00)%\n",
      "----------------------------------------\n",
      "0.5 5 0.5 (120269, 10) (120269,)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample retain and forget sets\n",
    "forget_ratios = [0.05, 0.2, 0.5]\n",
    "# Step 1: k-anonymize and prepare D_k\n",
    "ft_epochs_list = [5]\n",
    "m = int(len(y_train)*forget_ratio)\n",
    "idxs = np.arange(len(y_train))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "retain_idxs = idxs[m:]\n",
    "forget_idxs = idxs[:m]\n",
    "X_retain = X_train[retain_idxs]\n",
    "y_retain = y_train[retain_idxs]\n",
    "X_forget = X_train[forget_idxs]\n",
    "y_forget = y_train[forget_idxs]\n",
    "\n",
    "for forget_ratio in forget_ratios:\n",
    "    idxs = np.arange(len(y_train))\n",
    "    random.shuffle(idxs)\n",
    "    m = int(len(y_train)*forget_ratio)\n",
    "    retain_idxs = idxs[m:]\n",
    "    forget_idxs = idxs[:m]\n",
    "    X_retain = X_train[retain_idxs]\n",
    "    y_retain = y_train[retain_idxs]\n",
    "    X_forget = X_train[forget_idxs]\n",
    "    y_forget = y_train[forget_idxs]\n",
    "\n",
    "    for r in range(n_repeat):\n",
    "        ft_epochs_list = [5]\n",
    "        for ft_epochs in ft_epochs_list:\n",
    "            EPS = [0.5]\n",
    "            for eps in EPS:\n",
    "                dp_train_data = pd.read_csv('dp_data/GiveMeSomeCredit/dp_credit_eps={}.csv'.format(eps), sep=',')\n",
    "                # Drop useless columns\n",
    "                dp_train_data.dropna(inplace=True)\n",
    "                # convert the income column to 0 or 1 and then drop the column for the feature vectors\n",
    "                # creating the feature vector \n",
    "                X_train_dp = dp_train_data.drop('SeriousDlqin2yrs', axis =1)\n",
    "                # target values\n",
    "                y_train_dp = dp_train_data['SeriousDlqin2yrs'].values\n",
    "                # pass the data through the full_pipeline\n",
    "                X_train_dp = SC.fit_transform(X_train_dp)\n",
    "                print(forget_ratio, ft_epochs, eps, X_train_dp.shape, y_train_dp.shape)\n",
    "                train_accs_k = []\n",
    "                test_accs_k = []\n",
    "                mia_aucs_k = []\n",
    "                mia_advs_k = []\n",
    "                runtimes_train_k = []\n",
    "\n",
    "                train_accs_k_D = []\n",
    "                test_accs_k_D = []\n",
    "                mia_aucs_k_D = []\n",
    "                mia_advs_k_D = []\n",
    "                runtimes_train_k_D = []\n",
    "\n",
    "                retain_accs_k_ret = []\n",
    "                forget_accs_k_ret = []\n",
    "                test_accs_k_ret = []\n",
    "                mia_aucs_k_ret = []\n",
    "                mia_advs_k_ret = []\n",
    "                runtimes_train_k_ret = []\n",
    "\n",
    "                for r in range(n_repeat):\n",
    "                    model_k = copy.deepcopy(initial_model)\n",
    "                    t0 = time.time()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    model_k.fit(X_train_dp, y_train_dp)\n",
    "                    t1 = time.time()\n",
    "                    rt_train = t1- t0\n",
    "                    runtimes_train_k.append(rt_train)\n",
    "\n",
    "                    # Evaluate the model accuracy, and MIA\n",
    "                    # Accuracy\n",
    "                    train_acc = roc_auc_score(y_train_dp, model_k.predict_proba(X_train_dp)[:, 1])\n",
    "                    test_acc = roc_auc_score(y_test, model_k.predict_proba(X_test)[:, 1])\n",
    "                    train_accs_k.append(100.0*train_acc)\n",
    "                    test_accs_k.append(100.0*test_acc)\n",
    "                    #MIA\n",
    "                    idxs = np.arange(len(y_test))\n",
    "                    random.shuffle(idxs)\n",
    "                    rand_idxs = idxs[:m]\n",
    "\n",
    "                    test_preds = model_k.predict_proba(X_test)\n",
    "                    forget_preds = model_k.predict_proba(X_forget)\n",
    "\n",
    "                    # Convert class indices to one-hot encoding\n",
    "                    y_test_one_hot = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "                    y_forget_one_hot = encoder.transform(y_forget.reshape(-1, 1))\n",
    "\n",
    "                    loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "                    loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "                    attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                              loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                              train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "                    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                    mia_aucs_k.append(100.0*auc)\n",
    "                    mia_advs_k.append(100.0*adv)\n",
    "\n",
    "                    model_k_D = copy.deepcopy(initial_model)\n",
    "                    model_k_D.set_params(learning_rate = 0.5, n_estimators=ft_epochs)\n",
    "                    t0 = time.time()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    model_k_D.fit(X_train, y_train, xgb_model=model_k)\n",
    "                    t1 = time.time()\n",
    "                    rt = t1-t0\n",
    "                    runtimes_train_k_D.append(rt)\n",
    "\n",
    "                    # Evaluate the model accuracy, and MIA\n",
    "                    # Accuracy\n",
    "                    train_acc = roc_auc_score(y_train, model_k_D.predict_proba(X_train)[:, 1])\n",
    "                    test_acc = roc_auc_score(y_test, model_k_D.predict_proba(X_test)[:, 1])\n",
    "                    train_accs_k_D.append(100.0*train_acc)\n",
    "                    test_accs_k_D.append(100.0*test_acc)\n",
    "                    #MIA\n",
    "                    idxs = np.arange(len(y_test))\n",
    "                    random.shuffle(idxs)\n",
    "                    rand_idxs = idxs[:m]\n",
    "\n",
    "                    test_preds = model_k_D.predict_proba(X_test)\n",
    "                    forget_preds = model_k_D.predict_proba(X_forget)\n",
    "\n",
    "                    # Convert class indices to one-hot encoding\n",
    "                    y_test_one_hot = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "                    y_forget_one_hot = encoder.transform(y_forget.reshape(-1, 1))\n",
    "\n",
    "                    loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "                    loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "                    attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                              loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                              train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "                    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                    mia_aucs_k_D.append(100.0*auc)\n",
    "                    mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "                    model_k_ret = copy.deepcopy(initial_model)\n",
    "                    model_k_ret.set_params(learning_rate = 0.5, n_estimators=ft_epochs)\n",
    "                    t0 = time.time()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    model_k_ret.fit(X_retain, y_retain, xgb_model=model_k)\n",
    "                    t1 = time.time()\n",
    "                    rt = t1-t0\n",
    "                    runtimes_train_k_ret.append(rt)\n",
    "                    # Evaluate the model accuracy, and MIA\n",
    "                    # Accuracy\n",
    "                    retain_acc = roc_auc_score(y_retain, model_k_ret.predict_proba(X_retain)[:, 1])\n",
    "                    forget_acc = roc_auc_score(y_forget, model_k_ret.predict_proba(X_forget)[:, 1])\n",
    "                    test_acc = roc_auc_score(y_test, model_k_ret.predict_proba(X_test)[:, 1])\n",
    "                    retain_accs_k_ret.append(100.0*retain_acc)\n",
    "                    forget_accs_k_ret.append(100.0*forget_acc)\n",
    "                    test_accs_k_ret.append(100.0*test_acc)\n",
    "                    #MIA\n",
    "                    idxs = np.arange(len(y_test))\n",
    "                    random.shuffle(idxs)\n",
    "                    rand_idxs = idxs[:m]\n",
    "\n",
    "                    test_preds = model_k_ret.predict_proba(X_test)\n",
    "                    forget_preds = model_k_ret.predict_proba(X_forget)\n",
    "                    loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "                    loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget))])\n",
    "\n",
    "                    attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                                              loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                                              train_labels = y_forget, test_labels = y_test[rand_idxs])\n",
    "\n",
    "                    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                    mia_aucs_k_ret.append(100.0*auc)\n",
    "                    mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "                # Anonymizing D and training M_k on D_k\n",
    "                mean_train_k_time = np.mean(runtimes_train_k)\n",
    "                std_train_k_time = np.std(runtimes_train_k)\n",
    "                mean_train_k_acc = np.mean(train_accs_k)\n",
    "                std_train_k_acc = np.std(train_accs_k)\n",
    "                mean_test_k_acc = np.mean(test_accs_k)\n",
    "                std_test_k_acc = np.std(test_accs_k)\n",
    "                mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "                std_mia_k_auc = np.std(mia_aucs_k)\n",
    "                mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "                std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "                # Finetuning M_k on D\n",
    "                mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "                std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "                mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "                std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "                mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "                std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "                mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "                std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "                mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "                std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "                # Finetuning M_k on D_ret\n",
    "                mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "                std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "                mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "                std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "                mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "                std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "                mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "                std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "                mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "                std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "                mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "                std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "\n",
    "                # Print the results\n",
    "                print('----------------------------------------')\n",
    "                print('Epsilon=', eps, 'Fine-tuning epochs=', ft_epochs)\n",
    "                print('----------------------------------------')\n",
    "                print('-----Anonymizing D and training M_k on D_k-----')\n",
    "                print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "                print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "                print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "                print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "                print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "                print('-----Finetuning M_k on D-----')\n",
    "                print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "                print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "                print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "                print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "                print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "                print('-----Finetuning M_k on D_ret-----')\n",
    "                print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "                print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "                print('Forget accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "                print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "                print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "                print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "                print('----------------------------------------')\n",
    "\n",
    "                # Save to CSV\n",
    "                csv_anonymize_file_path = 'results/credit/xgb_mdp_eps={}_fr={}.csv'.format(eps, forget_ratio)\n",
    "                csv_finetune_D_file_path = 'results/credit/xgb_mdpd_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "                csv_finetune_D_ret_file_path = 'results/credit/xgb_mdpret_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "\n",
    "                # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "                with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                    writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "                    writer.writerow(['Train AUC', mean_train_k_acc, std_train_k_acc])\n",
    "                    writer.writerow(['Test AUC', mean_test_k_acc, std_test_k_acc])\n",
    "                    writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "                    writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "                with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                    writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "                    writer.writerow(['Train AUC', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "                    writer.writerow(['Test AUC', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "                    writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "                    writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "                with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                    writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "                    writer.writerow(['Retain AUC', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "                    writer.writerow(['Forget AUC', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "                    writer.writerow(['Test AUC', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "                    writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "                    writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a298f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
