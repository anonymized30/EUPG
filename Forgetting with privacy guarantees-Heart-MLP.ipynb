{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d3343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "from utils import *\n",
    "from datasets import *\n",
    "from mdav import *\n",
    "from train import *\n",
    "from models import *\n",
    "from attacks import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import Counter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c32be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "\n",
    "# Filter out ConvergenceWarning and FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_everything(seed=7):\n",
    "#     np.random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# seed_everything(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba412321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (56074, 11)\n",
      "X_train unique shape (56048, 11)\n",
      "X_test shape (13926, 11)\n",
      "X_test unique shape (13925, 11)\n",
      "X_retain shape (53271, 11)\n",
      "X_retain unique shape (53248, 11)\n",
      "X_forget shape (2803, 11)\n",
      "X_forget unique shape (2803, 11)\n",
      "Class=0, Count=28071, Percentage=50.06%\n",
      "Class=1, Count=28003, Percentage=49.94%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get dataset\n",
    "\n",
    "df=pd.read_csv('data/heart/cardio_train.csv', sep=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "split_ratio = 0.8  # 80% for the first DataFrame, 20% for the second\n",
    "# Perform the random split\n",
    "mask = np.random.rand(len(df)) < split_ratio\n",
    "trainset = df[mask]\n",
    "testset = df[~mask]\n",
    "# Reset the index of the new DataFrames if needed\n",
    "trainset.reset_index(drop=True, inplace=True)\n",
    "testset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "X_train = trainset.iloc[:,:-1].values\n",
    "y_train = trainset.iloc[:,-1].values\n",
    "X_test = testset.iloc[:,:-1].values\n",
    "y_test = testset.iloc[:,-1].values\n",
    "\n",
    "SC = StandardScaler()\n",
    "X_train = SC.fit_transform(X_train)\n",
    "X_test = SC.transform(X_test)\n",
    "\n",
    "num_features = X_train.shape[-1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "# Randomly sample retain and forget sets\n",
    "forget_ratio = 0.05\n",
    "m = int(len(y_train)*forget_ratio)\n",
    "idxs = np.arange(len(y_train))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "retain_idxs = idxs[m:]\n",
    "forget_idxs = idxs[:m]\n",
    "X_retain = X_train[retain_idxs]\n",
    "y_retain = y_train[retain_idxs]\n",
    "X_forget = X_train[forget_idxs]\n",
    "y_forget = y_train[forget_idxs]\n",
    "\n",
    "print('X_train shape', X_train.shape)\n",
    "print('X_train unique shape', np.unique(X_train, axis = 0).shape)\n",
    "print('X_test shape', X_test.shape)\n",
    "print('X_test unique shape', np.unique(X_test, axis = 0).shape)\n",
    "print('X_retain shape', X_retain.shape)\n",
    "print('X_retain unique shape', np.unique(X_retain, axis = 0).shape)\n",
    "print('X_forget shape', X_forget.shape)\n",
    "print('X_forget unique shape', np.unique(X_forget, axis = 0).shape)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.int64))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.int64))\n",
    "retain_dataset = TensorDataset(torch.tensor(X_retain, dtype=torch.float32), torch.tensor(y_retain, dtype=torch.int64))\n",
    "forget_dataset = TensorDataset(torch.tensor(X_forget, dtype=torch.float32), torch.tensor(y_forget, dtype=torch.int64))\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_model = MLPModel(num_features, 128, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-2\n",
    "n_repeat = 3\n",
    "max_epochs = 200\n",
    "patience = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecdf5bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training M on D time:198.27(±2.01) seconds\n",
      "Train accuracy:75.96(±0.14)%\n",
      "Test accuracy:72.35(±0.04)%\n",
      "MIA AUC:52.90(±0.27)%\n",
      "MIA Advantage:4.51(±0.34)%\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define and train M on D\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = copy.deepcopy(initial_model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    model = train_model(model, train_loader, test_loader, criterion, optimizer, \n",
    "                        max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                        patience = patience)\n",
    "\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    model.eval()\n",
    "    # Accuracy\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "    train_accs.append(100.0*train_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    idxs = np.arange(len(test_dataset))\n",
    "    random.shuffle(idxs)\n",
    "    rand_idxs = idxs[:m]\n",
    "    logits_test, loss_test, test_labels = compute_attack_components(model, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components(model, forget_loader)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/heart/mlp_m_d_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Train Accuracy', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test Accuracy', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3aa61c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining M on D_ret time:186.73(±0.46) seconds\n",
      "Retain accuracy:75.54(±0.03)%\n",
      "Forget accuracy:72.14(±0.20)%\n",
      "Test accuracy:72.77(±0.20)%\n",
      "MIA AUC:50.12(±0.18)%\n",
      "MIA Advantage:1.99(±0.34)%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train M_retain on D_retain\n",
    "retain_accs = []\n",
    "forget_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    model_ret = copy.deepcopy(initial_model)\n",
    "    optimizer = optim.Adam(model_ret.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    model_ret = train_model(model_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                    max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                        patience = patience)\n",
    "\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    model_ret.eval()\n",
    "    # Accuracy\n",
    "    retain_acc = accuracy(model_ret, retain_loader)\n",
    "    test_acc = accuracy(model_ret, test_loader)\n",
    "    forget_acc = accuracy(model_ret, forget_loader)\n",
    "    retain_accs.append(100.0*retain_acc)\n",
    "    forget_accs.append(100.0*forget_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    logits_test, loss_test, test_labels = compute_attack_components(model_ret, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components(model_ret, forget_loader)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "    \n",
    "\n",
    "mean_retrain_runtime = np.mean(runtimes)\n",
    "std_retrain_runtime = np.std(runtimes)\n",
    "mean_retain_acc = np.mean(retain_accs)\n",
    "std_retain_acc = np.std(retain_accs)\n",
    "mean_forget_acc = np.mean(forget_accs)\n",
    "std_forget_acc = np.std(forget_accs)\n",
    "mean_retrain_test_acc = np.mean(test_accs)\n",
    "std_retrain_test_acc = np.std(test_accs)\n",
    "mean_retrain_mia_auc = np.mean(mia_aucs)\n",
    "std_retrain_mia_auc = np.std(mia_aucs)\n",
    "mean_retrain_mia_adv = np.mean(mia_advs)\n",
    "std_retrain_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Retraining M on D_ret time:{:0.2f}(±{:0.2f}) seconds'.format(mean_retrain_runtime, std_retrain_runtime))\n",
    "print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_retain_acc, std_retain_acc))\n",
    "print('Forget accuracy:{:0.2f}(±{:0.2f})%'.format(mean_forget_acc, std_forget_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_retrain_test_acc, std_retrain_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_auc, std_retrain_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_adv, std_retrain_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_retrain_file_path = 'results/heart/mlp_mret_dret_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "with open(csv_retrain_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Retraining Time', mean_retrain_runtime, std_retrain_runtime])\n",
    "    writer.writerow(['Retain Accuracy', mean_retain_acc, std_retain_acc])\n",
    "    writer.writerow(['Forget Accuracy', mean_forget_acc, std_forget_acc])\n",
    "    writer.writerow(['Test Accuracy', mean_retrain_test_acc, std_retrain_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_retrain_mia_auc, std_retrain_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_retrain_mia_adv, std_retrain_mia_adv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9287fc3",
   "metadata": {},
   "source": [
    "# k-anonymity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7487244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████▍ | 54903/56074 [01:29<00:00, 7684.86it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 56074/56074 [01:30<00:00, 621.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 18691\n",
      "Mean of mean distances to centroids: 3.9142376999754136\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 3 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:90.69(±0.00)\n",
      "Training M_k on D_k time:195.12(±1.16)\n",
      "Train accuracy:73.81(±0.03)%\n",
      "Test accuracy:72.65(±0.07)%\n",
      "MIA AUC:51.42(±0.14)%\n",
      "MIA Advantage:3.31(±0.13)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.08(±0.06)\n",
      "Train accuracy:73.74(±0.08)%\n",
      "Test accuracy:73.77(±0.08)%\n",
      "MIA AUC:50.46(±0.41)%\n",
      "MIA Advantage:2.63(±0.77)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.78(±0.04) seconds\n",
      "Retain accuracy:73.73(±0.08)%\n",
      "Forget accuracy:72.45(±0.24)%\n",
      "Test accuracy:73.87(±0.03)%\n",
      "MIA AUC:51.03(±0.68)%\n",
      "MIA Advantage:2.95(±0.64)%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████▌| 55785/56074 [00:56<00:00, 12483.04it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 56074/56074 [00:57<00:00, 976.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 11214\n",
      "Mean of mean distances to centroids: 3.905743138894533\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 5 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:57.70(±0.00)\n",
      "Training M_k on D_k time:193.46(±0.21)\n",
      "Train accuracy:73.21(±0.12)%\n",
      "Test accuracy:72.41(±0.18)%\n",
      "MIA AUC:51.65(±0.73)%\n",
      "MIA Advantage:3.82(±1.06)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.06(±0.03)\n",
      "Train accuracy:73.67(±0.03)%\n",
      "Test accuracy:73.64(±0.12)%\n",
      "MIA AUC:50.43(±0.22)%\n",
      "MIA Advantage:3.90(±0.70)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.67(±0.03) seconds\n",
      "Retain accuracy:73.82(±0.08)%\n",
      "Forget accuracy:72.42(±0.28)%\n",
      "Test accuracy:73.71(±0.06)%\n",
      "MIA AUC:51.04(±0.22)%\n",
      "MIA Advantage:2.31(±0.37)%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████▌ | 54940/56074 [00:28<00:00, 16529.46it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 56074/56074 [00:28<00:00, 1949.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 5607\n",
      "Mean of mean distances to centroids: 3.891002645095943\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 10 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:28.90(±0.00)\n",
      "Training M_k on D_k time:193.18(±0.41)\n",
      "Train accuracy:73.09(±0.05)%\n",
      "Test accuracy:73.14(±0.08)%\n",
      "MIA AUC:51.15(±0.11)%\n",
      "MIA Advantage:4.00(±0.26)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:4.98(±0.09)\n",
      "Train accuracy:73.58(±0.10)%\n",
      "Test accuracy:73.82(±0.10)%\n",
      "MIA AUC:50.81(±0.68)%\n",
      "MIA Advantage:3.14(±0.61)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.66(±0.03) seconds\n",
      "Retain accuracy:73.76(±0.06)%\n",
      "Forget accuracy:72.93(±0.21)%\n",
      "Test accuracy:73.87(±0.07)%\n",
      "MIA AUC:51.10(±0.25)%\n",
      "MIA Advantage:2.27(±0.32)%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████    | 53080/56074 [00:14<00:00, 19980.51it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 56074/56074 [00:14<00:00, 3868.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 2803\n",
      "Mean of mean distances to centroids: 3.8704355821712815\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 20 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:14.57(±0.00)\n",
      "Training M_k on D_k time:193.24(±0.29)\n",
      "Train accuracy:72.28(±0.19)%\n",
      "Test accuracy:72.45(±0.46)%\n",
      "MIA AUC:50.93(±0.54)%\n",
      "MIA Advantage:3.58(±0.44)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.08(±0.04)\n",
      "Train accuracy:73.61(±0.08)%\n",
      "Test accuracy:73.80(±0.10)%\n",
      "MIA AUC:51.04(±0.78)%\n",
      "MIA Advantage:2.66(±0.35)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.69(±0.03) seconds\n",
      "Retain accuracy:73.53(±0.16)%\n",
      "Forget accuracy:72.32(±0.36)%\n",
      "Test accuracy:73.66(±0.14)%\n",
      "MIA AUC:50.80(±0.27)%\n",
      "MIA Advantage:2.97(±0.57)%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████▌           | 47280/56074 [00:03<00:00, 29147.06it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 56074/56074 [00:03<00:00, 15372.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 700\n",
      "Mean of mean distances to centroids: 3.814035685806171\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 80 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:3.67(±0.00)\n",
      "Training M_k on D_k time:195.13(±2.54)\n",
      "Train accuracy:70.82(±0.28)%\n",
      "Test accuracy:71.45(±0.23)%\n",
      "MIA AUC:50.77(±0.03)%\n",
      "MIA Advantage:3.21(±0.44)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.07(±0.09)\n",
      "Train accuracy:73.60(±0.06)%\n",
      "Test accuracy:73.94(±0.15)%\n",
      "MIA AUC:50.37(±0.04)%\n",
      "MIA Advantage:2.58(±0.55)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.90(±0.30) seconds\n",
      "Retain accuracy:73.53(±0.06)%\n",
      "Forget accuracy:72.47(±0.20)%\n",
      "Test accuracy:73.86(±0.12)%\n",
      "MIA AUC:50.88(±0.57)%\n",
      "MIA Advantage:3.20(±1.05)%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████▎               | 44000/56074 [00:01<00:00, 38840.42it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 56074/56074 [00:01<00:00, 30124.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 350\n",
      "Mean of mean distances to centroids: 3.760483100624267\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 160 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:1.88(±0.00)\n",
      "Training M_k on D_k time:201.51(±3.82)\n",
      "Train accuracy:70.96(±0.10)%\n",
      "Test accuracy:71.54(±0.07)%\n",
      "MIA AUC:50.45(±0.15)%\n",
      "MIA Advantage:2.72(±0.24)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.45(±0.17)\n",
      "Train accuracy:73.51(±0.17)%\n",
      "Test accuracy:73.84(±0.10)%\n",
      "MIA AUC:50.84(±0.23)%\n",
      "MIA Advantage:2.93(±0.65)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.82(±0.16) seconds\n",
      "Retain accuracy:73.67(±0.06)%\n",
      "Forget accuracy:72.66(±0.26)%\n",
      "Test accuracy:73.94(±0.04)%\n",
      "MIA AUC:50.55(±0.57)%\n",
      "MIA Advantage:2.76(±0.25)%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████▎         | 48640/56074 [00:00<00:00, 75257.19it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 56074/56074 [00:00<00:00, 58874.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 175\n",
      "Mean of mean distances to centroids: 3.713880157518731\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 320 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:0.97(±0.00)\n",
      "Training M_k on D_k time:196.22(±1.06)\n",
      "Train accuracy:70.45(±0.39)%\n",
      "Test accuracy:70.78(±0.50)%\n",
      "MIA AUC:51.07(±0.51)%\n",
      "MIA Advantage:2.95(±0.44)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.11(±0.03)\n",
      "Train accuracy:73.51(±0.07)%\n",
      "Test accuracy:73.97(±0.01)%\n",
      "MIA AUC:50.08(±0.12)%\n",
      "MIA Advantage:3.25(±0.53)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.86(±0.13) seconds\n",
      "Retain accuracy:73.60(±0.01)%\n",
      "Forget accuracy:72.35(±0.20)%\n",
      "Test accuracy:73.94(±0.08)%\n",
      "MIA AUC:50.73(±0.71)%\n",
      "MIA Advantage:3.46(±0.32)%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████▉                            | 34560/56074 [00:00<00:00, 94145.31it/s]D:\\machine unlearning\\josep idea\\mdav.py:103: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  data_k = np.vstack(np.repeat(c.mean(0).reshape(1, -1), len(c), axis = 0) for c in clusters)\n",
      "100%|████████████████████████████████████████████████████████████████████████| 56074/56074 [00:00<00:00, 114081.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 87\n",
      "Mean of mean distances to centroids: 3.638771284768852\n",
      "Shape of X_train_k:(56074, 11), y_train_k:(56074,)\n",
      "----------------------------------------\n",
      "k= 640 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_k on D_k-----\n",
      "Anonymizing D time:0.50(±0.00)\n",
      "Training M_k on D_k time:195.96(±2.21)\n",
      "Train accuracy:68.59(±0.23)%\n",
      "Test accuracy:69.09(±0.28)%\n",
      "MIA AUC:51.38(±0.96)%\n",
      "MIA Advantage:3.78(±1.08)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.10(±0.04)\n",
      "Train accuracy:73.44(±0.01)%\n",
      "Test accuracy:74.02(±0.10)%\n",
      "MIA AUC:50.30(±0.25)%\n",
      "MIA Advantage:3.25(±0.35)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.74(±0.04) seconds\n",
      "Retain accuracy:73.51(±0.07)%\n",
      "Forget accuracy:72.29(±0.19)%\n",
      "Test accuracy:73.92(±0.05)%\n",
      "MIA AUC:50.41(±0.27)%\n",
      "MIA Advantage:2.51(±0.44)%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1: k-anonymize and prepare D_k\n",
    "ft_epochs_list = [5]\n",
    "for ft_epochs in ft_epochs_list:\n",
    "    K = [3, 5, 10, 20, 80, 160, 320, 640]\n",
    "    for k in K:\n",
    "        runtimes_k = []\n",
    "        t0 = time.time()\n",
    "        centroids, clusters, labels, X_train_k, y_train_k = mdav(copy.deepcopy(X_train), copy.deepcopy(y_train), k)\n",
    "        print_stats(clusters, centroids)\n",
    "        print('Shape of X_train_k:{}, y_train_k:{}'.format(X_train_k.shape, y_train_k.shape))\n",
    "         # Create TensorDatasets\n",
    "        train_dataset_k = TensorDataset(torch.tensor(X_train_k, dtype=torch.float32), torch.tensor(y_train_k, dtype=torch.int64))\n",
    "        train_loader_k = DataLoader(train_dataset_k, batch_size=batch_size, shuffle=True)\n",
    "        t1 = time.time()\n",
    "        rt_k = t1- t0\n",
    "        runtimes_k.append(rt_k)\n",
    "\n",
    "        train_accs_k = []\n",
    "        test_accs_k = []\n",
    "        mia_aucs_k = []\n",
    "        mia_advs_k = []\n",
    "        runtimes_train_k = []\n",
    "\n",
    "        train_accs_k_D = []\n",
    "        test_accs_k_D = []\n",
    "        mia_aucs_k_D = []\n",
    "        mia_advs_k_D = []\n",
    "        runtimes_train_k_D = []\n",
    "\n",
    "        retain_accs_k_ret = []\n",
    "        forget_accs_k_ret = []\n",
    "        test_accs_k_ret = []\n",
    "        mia_aucs_k_ret = []\n",
    "        mia_advs_k_ret = []\n",
    "        runtimes_train_k_ret = []\n",
    "\n",
    "        for r in range(n_repeat):\n",
    "            torch.cuda.empty_cache()\n",
    "            model_k = copy.deepcopy(initial_model)\n",
    "            optimizer = optim.Adam(model_k.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k = train_model(model_k, train_loader_k, test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt_train = t1- t0\n",
    "            runtimes_train_k.append(rt_train)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k, train_loader)\n",
    "            test_acc = accuracy(model_k, test_loader)\n",
    "            train_accs_k.append(100.0*train_acc)\n",
    "            test_accs_k.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k.append(100.0*auc)\n",
    "            mia_advs_k.append(100.0*adv)\n",
    "\n",
    "            model_k_D = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_D.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_D = train_model(model_k_D, train_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_D.append(rt)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_D.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k_D, train_loader)\n",
    "            test_acc = accuracy(model_k_D, test_loader)\n",
    "            train_accs_k_D.append(100.0*train_acc)\n",
    "            test_accs_k_D.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_D, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_D, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_D.append(100.0*auc)\n",
    "            mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "            model_k_ret = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_ret.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_ret = train_model(model_k_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_ret.append(rt)\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_ret.eval()\n",
    "            #Accuracy\n",
    "            retain_acc = accuracy(model_k_ret, retain_loader)\n",
    "            forget_acc = accuracy(model_k_ret, forget_loader)\n",
    "            test_acc = accuracy(model_k_ret, test_loader)\n",
    "            retain_accs_k_ret.append(100.0*retain_acc)\n",
    "            forget_accs_k_ret.append(100.0*forget_acc)\n",
    "            test_accs_k_ret.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_ret, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_ret, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_ret.append(100.0*auc)\n",
    "            mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "        # Anonymizing D and training M_k on D_k\n",
    "        mean_anonymize_time = np.mean(runtimes_k)\n",
    "        std_anonymize_time = np.std(runtimes_k)\n",
    "        mean_train_k_time = np.mean(runtimes_train_k)\n",
    "        std_train_k_time = np.std(runtimes_train_k)\n",
    "        mean_train_k_acc = np.mean(train_accs_k)\n",
    "        std_train_k_acc = np.std(train_accs_k)\n",
    "        mean_test_k_acc = np.mean(test_accs_k)\n",
    "        std_test_k_acc = np.std(test_accs_k)\n",
    "        mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "        std_mia_k_auc = np.std(mia_aucs_k)\n",
    "        mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "        std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "        # Finetuning M_k on D\n",
    "        mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "        std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "        mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "        std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "        mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "        std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "        mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "        std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "        mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "        std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "        # Finetuning M_k on D_ret\n",
    "        mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "        std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "        mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "        std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "        mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "        std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "        mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "        std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "        mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "        std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "        mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "        std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "\n",
    "\n",
    "        # Print the results\n",
    "        print('----------------------------------------')\n",
    "        print('k=', k, 'Fine-tuning epochs=', ft_epochs)\n",
    "        print('----------------------------------------')\n",
    "        print('-----Anonymizing D and training M_k on D_k-----')\n",
    "        print('Anonymizing D time:{:0.2f}(±{:0.2f})'.format(mean_anonymize_time, std_anonymize_time))\n",
    "        print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D-----')\n",
    "        print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D_ret-----')\n",
    "        print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "        print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "        print('Forget accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_anonymize_file_path = 'results/heart/mlp_mk={}_dk_fr={}.csv'.format(k, forget_ratio)\n",
    "        csv_finetune_D_file_path = 'results/heart/mlp_mk={}_d_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "        csv_finetune_D_ret_file_path = 'results/heart/mlp_mk={}_dret_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "\n",
    "        # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "        with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Anonymizing Time', mean_anonymize_time, std_anonymize_time])\n",
    "            writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "            writer.writerow(['Train Accuracy', mean_train_k_acc, std_train_k_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_test_k_acc, std_test_k_acc])\n",
    "            writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "        with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "            writer.writerow(['Train Accuracy', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "        with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "            writer.writerow(['Retain Accuracy', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "            writer.writerow(['Forget Accuracy', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e849dbf",
   "metadata": {},
   "source": [
    "# Differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d4e10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Epsilon= 0.5 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:254.24(±8.19)\n",
      "Train accuracy:50.36(±0.76)%\n",
      "Test accuracy:50.59(±0.54)%\n",
      "MIA AUC:50.79(±0.76)%\n",
      "MIA Advantage:2.60(±0.34)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.08(±0.11)\n",
      "Train accuracy:73.20(±0.05)%\n",
      "Test accuracy:73.52(±0.03)%\n",
      "MIA AUC:50.86(±0.73)%\n",
      "MIA Advantage:2.71(±0.43)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.86(±0.08) seconds\n",
      "Retain accuracy:73.42(±0.22)%\n",
      "Forget accuracy:72.20(±0.19)%\n",
      "Test accuracy:73.73(±0.43)%\n",
      "MIA AUC:50.36(±0.61)%\n",
      "MIA Advantage:3.03(±0.64)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 2.5 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:253.10(±0.13)\n",
      "Train accuracy:51.51(±0.87)%\n",
      "Test accuracy:51.74(±0.93)%\n",
      "MIA AUC:50.58(±0.18)%\n",
      "MIA Advantage:2.68(±0.13)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.18(±0.04)\n",
      "Train accuracy:73.28(±0.05)%\n",
      "Test accuracy:73.79(±0.12)%\n",
      "MIA AUC:51.04(±1.00)%\n",
      "MIA Advantage:3.60(±0.52)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.85(±0.04) seconds\n",
      "Retain accuracy:73.32(±0.13)%\n",
      "Forget accuracy:72.18(±0.30)%\n",
      "Test accuracy:73.60(±0.07)%\n",
      "MIA AUC:50.97(±0.50)%\n",
      "MIA Advantage:3.16(±0.56)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 5.0 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:248.67(±2.94)\n",
      "Train accuracy:55.40(±0.81)%\n",
      "Test accuracy:55.59(±0.90)%\n",
      "MIA AUC:50.88(±0.52)%\n",
      "MIA Advantage:2.66(±0.12)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.19(±0.01)\n",
      "Train accuracy:73.39(±0.10)%\n",
      "Test accuracy:73.87(±0.20)%\n",
      "MIA AUC:50.29(±0.27)%\n",
      "MIA Advantage:2.43(±0.25)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.79(±0.14) seconds\n",
      "Retain accuracy:73.61(±0.02)%\n",
      "Forget accuracy:72.17(±0.21)%\n",
      "Test accuracy:73.83(±0.16)%\n",
      "MIA AUC:50.78(±0.61)%\n",
      "MIA Advantage:3.01(±0.51)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 25.0 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:246.85(±4.45)\n",
      "Train accuracy:59.01(±1.18)%\n",
      "Test accuracy:58.95(±1.26)%\n",
      "MIA AUC:50.97(±0.29)%\n",
      "MIA Advantage:2.85(±0.23)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:4.96(±0.21)\n",
      "Train accuracy:73.05(±0.32)%\n",
      "Test accuracy:73.46(±0.28)%\n",
      "MIA AUC:50.94(±0.29)%\n",
      "MIA Advantage:2.88(±0.31)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.81(±0.02) seconds\n",
      "Retain accuracy:73.22(±0.17)%\n",
      "Forget accuracy:72.09(±0.21)%\n",
      "Test accuracy:73.60(±0.14)%\n",
      "MIA AUC:50.03(±0.03)%\n",
      "MIA Advantage:3.38(±0.73)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 50.0 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:242.97(±0.06)\n",
      "Train accuracy:62.71(±0.13)%\n",
      "Test accuracy:62.53(±0.50)%\n",
      "MIA AUC:50.64(±0.30)%\n",
      "MIA Advantage:2.85(±0.15)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.10(±0.02)\n",
      "Train accuracy:73.29(±0.13)%\n",
      "Test accuracy:73.69(±0.19)%\n",
      "MIA AUC:51.60(±0.44)%\n",
      "MIA Advantage:2.74(±0.31)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.75(±0.01) seconds\n",
      "Retain accuracy:73.20(±0.36)%\n",
      "Forget accuracy:71.85(±0.25)%\n",
      "Test accuracy:73.18(±0.46)%\n",
      "MIA AUC:50.60(±0.68)%\n",
      "MIA Advantage:3.02(±0.48)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 100.0 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:241.97(±0.24)\n",
      "Train accuracy:62.80(±0.45)%\n",
      "Test accuracy:62.82(±0.30)%\n",
      "MIA AUC:50.52(±0.09)%\n",
      "MIA Advantage:3.52(±0.44)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.05(±0.06)\n",
      "Train accuracy:73.35(±0.04)%\n",
      "Test accuracy:73.62(±0.05)%\n",
      "MIA AUC:50.90(±0.16)%\n",
      "MIA Advantage:3.22(±0.79)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.70(±0.05) seconds\n",
      "Retain accuracy:73.05(±0.28)%\n",
      "Forget accuracy:71.63(±0.14)%\n",
      "Test accuracy:73.22(±0.48)%\n",
      "MIA AUC:50.37(±0.30)%\n",
      "MIA Advantage:2.71(±0.43)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 250 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:243.40(±1.47)\n",
      "Train accuracy:63.54(±0.18)%\n",
      "Test accuracy:63.22(±0.09)%\n",
      "MIA AUC:51.41(±0.23)%\n",
      "MIA Advantage:3.20(±0.60)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.07(±0.01)\n",
      "Train accuracy:73.36(±0.16)%\n",
      "Test accuracy:73.52(±0.21)%\n",
      "MIA AUC:51.09(±0.29)%\n",
      "MIA Advantage:3.00(±0.49)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.70(±0.03) seconds\n",
      "Retain accuracy:73.27(±0.19)%\n",
      "Forget accuracy:71.84(±0.47)%\n",
      "Test accuracy:73.49(±0.19)%\n",
      "MIA AUC:51.19(±0.09)%\n",
      "MIA Advantage:3.08(±0.17)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 500 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:242.10(±0.19)\n",
      "Train accuracy:63.61(±0.53)%\n",
      "Test accuracy:63.48(±0.42)%\n",
      "MIA AUC:51.09(±0.92)%\n",
      "MIA Advantage:2.75(±0.77)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.09(±0.00)\n",
      "Train accuracy:73.49(±0.11)%\n",
      "Test accuracy:73.73(±0.28)%\n",
      "MIA AUC:50.32(±0.38)%\n",
      "MIA Advantage:2.59(±0.22)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.81(±0.00) seconds\n",
      "Retain accuracy:73.11(±0.32)%\n",
      "Forget accuracy:71.79(±0.34)%\n",
      "Test accuracy:73.24(±0.39)%\n",
      "MIA AUC:50.53(±0.12)%\n",
      "MIA Advantage:2.81(±0.98)%\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Epsilon= 1000 Fine-tuning epochs= 5\n",
      "----------------------------------------\n",
      "-----Anonymizing D and training M_dp on D_dp-----\n",
      "Training M_k on D_k time:242.34(±0.11)\n",
      "Train accuracy:63.89(±0.25)%\n",
      "Test accuracy:63.35(±0.42)%\n",
      "MIA AUC:50.37(±0.16)%\n",
      "MIA Advantage:2.50(±0.23)%\n",
      "-----Finetuning M_k on D-----\n",
      "Training M_k on D time:5.09(±0.01)\n",
      "Train accuracy:73.30(±0.05)%\n",
      "Test accuracy:73.49(±0.28)%\n",
      "MIA AUC:51.11(±0.09)%\n",
      "MIA Advantage:2.78(±0.48)%\n",
      "-----Finetuning M_k on D_ret-----\n",
      "Finetuning M_k on D_retain time:4.73(±0.06) seconds\n",
      "Retain accuracy:73.21(±0.10)%\n",
      "Forget accuracy:71.97(±0.43)%\n",
      "Test accuracy:73.44(±0.33)%\n",
      "MIA AUC:51.30(±0.49)%\n",
      "MIA Advantage:3.54(±0.97)%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1: k-anonymize and prepare D_k\n",
    "ft_epochs_list = [5]\n",
    "for ft_epochs in ft_epochs_list:\n",
    "    EPS = [0.5, 2.5, 5., 25., 50., 100., 250, 500, 1000]\n",
    "    for eps in EPS:\n",
    "        dp_train_data = pd.read_csv('dp_data/heart/dp_heart_eps={}.csv'.format(eps), sep=',')\n",
    "        # Drop useless columns\n",
    "        dp_train_data.dropna(inplace=True)\n",
    "        # convert the income column to 0 or 1 and then drop the column for the feature vectors\n",
    "        # creating the feature vector \n",
    "        X_train_dp = dp_train_data.drop('cardio', axis =1)\n",
    "        # target values\n",
    "        y_train_dp = dp_train_data['cardio'].values\n",
    "        # pass the data through the full_pipeline\n",
    "        X_train_dp = SC.fit_transform(X_train_dp)\n",
    "        # Create TensorDatasets\n",
    "        train_dataset_k = TensorDataset(torch.tensor(X_train_dp, dtype=torch.float32), torch.tensor(y_train_dp, dtype=torch.int64))\n",
    "        train_loader_k = DataLoader(train_dataset_k, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        train_accs_k = []\n",
    "        test_accs_k = []\n",
    "        mia_aucs_k = []\n",
    "        mia_advs_k = []\n",
    "        runtimes_train_k = []\n",
    "\n",
    "        train_accs_k_D = []\n",
    "        test_accs_k_D = []\n",
    "        mia_aucs_k_D = []\n",
    "        mia_advs_k_D = []\n",
    "        runtimes_train_k_D = []\n",
    "\n",
    "        retain_accs_k_ret = []\n",
    "        forget_accs_k_ret = []\n",
    "        test_accs_k_ret = []\n",
    "        mia_aucs_k_ret = []\n",
    "        mia_advs_k_ret = []\n",
    "        runtimes_train_k_ret = []\n",
    "\n",
    "        for r in range(n_repeat):\n",
    "            torch.cuda.empty_cache()\n",
    "            model_k = copy.deepcopy(initial_model)\n",
    "            optimizer = optim.Adam(model_k.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k = train_model(model_k, train_loader_k, test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt_train = t1- t0\n",
    "            runtimes_train_k.append(rt_train)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k, train_loader)\n",
    "            test_acc = accuracy(model_k, test_loader)\n",
    "            train_accs_k.append(100.0*train_acc)\n",
    "            test_accs_k.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            idxs = np.arange(len(test_dataset))\n",
    "            random.shuffle(idxs)\n",
    "            rand_idxs = idxs[:m]\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k.append(100.0*auc)\n",
    "            mia_advs_k.append(100.0*adv)\n",
    "\n",
    "            model_k_D = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_D.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_D = train_model(model_k_D, train_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_D.append(rt)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_D.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k_D, train_loader)\n",
    "            test_acc = accuracy(model_k_D, test_loader)\n",
    "            train_accs_k_D.append(100.0*train_acc)\n",
    "            test_accs_k_D.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_D, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_D, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_D.append(100.0*auc)\n",
    "            mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "            model_k_ret = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_ret.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_ret = train_model(model_k_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_ret.append(rt)\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_ret.eval()\n",
    "            #Accuracy\n",
    "            retain_acc = accuracy(model_k_ret, retain_loader)\n",
    "            forget_acc = accuracy(model_k_ret, forget_loader)\n",
    "            test_acc = accuracy(model_k_ret, test_loader)\n",
    "            retain_accs_k_ret.append(100.0*retain_acc)\n",
    "            forget_accs_k_ret.append(100.0*forget_acc)\n",
    "            test_accs_k_ret.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_ret, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_ret, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_ret.append(100.0*auc)\n",
    "            mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "        # Anonymizing D and training M_k on D_k\n",
    "        mean_train_k_time = np.mean(runtimes_train_k)\n",
    "        std_train_k_time = np.std(runtimes_train_k)\n",
    "        mean_train_k_acc = np.mean(train_accs_k)\n",
    "        std_train_k_acc = np.std(train_accs_k)\n",
    "        mean_test_k_acc = np.mean(test_accs_k)\n",
    "        std_test_k_acc = np.std(test_accs_k)\n",
    "        mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "        std_mia_k_auc = np.std(mia_aucs_k)\n",
    "        mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "        std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "        # Finetuning M_k on D\n",
    "        mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "        std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "        mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "        std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "        mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "        std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "        mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "        std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "        mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "        std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "        # Finetuning M_k on D_ret\n",
    "        mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "        std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "        mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "        std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "        mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "        std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "        mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "        std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "        mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "        std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "        mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "        std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "        # Print the results\n",
    "        print('----------------------------------------')\n",
    "        print('Epsilon=', eps, 'Fine-tuning epochs=', ft_epochs)\n",
    "        print('----------------------------------------')\n",
    "        print('-----Anonymizing D and training M_dp on D_dp-----')\n",
    "        print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D-----')\n",
    "        print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D_ret-----')\n",
    "        print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "        print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "        print('Forget accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_anonymize_file_path = 'results/heart/mlp_mdp_eps={}_fr={}.csv'.format(eps, forget_ratio)\n",
    "        csv_finetune_D_file_path = 'results/heart/mlp_mdpd_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "        csv_finetune_D_ret_file_path = 'results/heart/mlp_mdpret_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "\n",
    "        # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "        with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "            writer.writerow(['Train Accuracy', mean_train_k_acc, std_train_k_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_test_k_acc, std_test_k_acc])\n",
    "            writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "        with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "            writer.writerow(['Train Accuracy', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "        with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "            writer.writerow(['Retain Accuracy', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "            writer.writerow(['Forget Accuracy', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
