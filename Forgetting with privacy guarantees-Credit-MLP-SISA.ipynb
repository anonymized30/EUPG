{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d3343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "from utils import *\n",
    "from datasets import *\n",
    "from mdav import *\n",
    "from train import *\n",
    "from models import *\n",
    "from attacks import *\n",
    "\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1531e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "\n",
    "# Filter out ConvergenceWarning and FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_everything(seed=7):\n",
    "#     np.random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# seed_everything(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba412321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get dataset\n",
    "\n",
    "df=pd.read_csv('data/GiveMeSomeCredit/cs-training.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "y = df['SeriousDlqin2yrs'].values\n",
    "df.drop(df[['SeriousDlqin2yrs']],axis=1,inplace=True)\n",
    "X = df.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "SC = StandardScaler()\n",
    "X_train = SC.fit_transform(X_train)\n",
    "X_test = SC.transform(X_test)\n",
    "\n",
    "# Sharding data\n",
    "forget_ratio = 0.8\n",
    "# Divide X_train into 5 equal shards\n",
    "num_shards = 5\n",
    "shard_size = len(y_train) // num_shards\n",
    "X_shards = []\n",
    "y_shards = []\n",
    "retain_sets_X = []\n",
    "retain_sets_y = []\n",
    "forget_sets_X = []\n",
    "forget_sets_y = []\n",
    "\n",
    "for i in range(num_shards):\n",
    "    # Calculate indices for slicing\n",
    "    start_idx = i * shard_size\n",
    "    end_idx = start_idx + shard_size if i < num_shards - 1 else len(y_train)\n",
    "    \n",
    "    # Slice the data to create shards\n",
    "    X_shard = X_train[start_idx:end_idx]\n",
    "    y_shard = y_train[start_idx:end_idx]\n",
    "    \n",
    "    X_shards.append(X_shard)\n",
    "    y_shards.append(y_shard)\n",
    "    \n",
    "    # Shuffle indices for random sampling\n",
    "    idxs = np.arange(len(y_shard))\n",
    "    random.shuffle(idxs)\n",
    "    m = int(len(y_shard) * forget_ratio)  # 5% forget ratio\n",
    "    \n",
    "    # Split indices for retain and forget sets\n",
    "    retain_idxs = idxs[m:]\n",
    "    forget_idxs = idxs[:m]\n",
    "    \n",
    "    # Create retain and forget sets for the shard\n",
    "    X_retain = X_shard[retain_idxs]\n",
    "    y_retain = y_shard[retain_idxs]\n",
    "    X_forget = X_shard[forget_idxs]\n",
    "    y_forget = y_shard[forget_idxs]\n",
    "    \n",
    "    retain_sets_X.append(X_retain)\n",
    "    retain_sets_y.append(y_retain)\n",
    "    forget_sets_X.append(X_forget)\n",
    "    forget_sets_y.append(y_forget)\n",
    "\n",
    "\n",
    "# Convert each shard's train, retain and forget sets into TensorDatasets\n",
    "train_datasets = [\n",
    "    TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.int64))\n",
    "    for X, y in zip(X_shards, y_shards)\n",
    "]\n",
    "\n",
    "\n",
    "retain_datasets = [\n",
    "    TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.int64))\n",
    "    for X, y in zip(retain_sets_X, retain_sets_y)\n",
    "]\n",
    "\n",
    "forget_datasets = [\n",
    "    TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.int64))\n",
    "    for X, y in zip(forget_sets_X, forget_sets_y)\n",
    "]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Create DataLoader instances for each retain and forget dataset\n",
    "train_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in train_datasets]\n",
    "retain_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in retain_datasets]\n",
    "forget_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=False) for dataset in forget_datasets]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.int64))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.int64))\n",
    "\n",
    "retain_dataset = torch.utils.data.ConcatDataset(retain_datasets)\n",
    "forget_dataset = torch.utils.data.ConcatDataset(forget_datasets)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_features = X_train.shape[-1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_model = MLPModel(num_features, 256, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "n_repeat = 3\n",
    "max_epochs = 200\n",
    "patience = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd14a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define and train M on D\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    models = []\n",
    "    t0 = time.time()\n",
    "    for i in range(num_shards):\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        model = train_model(model, train_loaders[i], test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "        models.append(model)\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    # Accuracy\n",
    "    train_acc = auc_score_with_majority_voting(models, train_loader)\n",
    "    test_acc = auc_score_with_majority_voting(models, test_loader)\n",
    "    train_accs.append(100.0*train_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    idxs = np.arange(len(test_dataset))\n",
    "    random.shuffle(idxs)\n",
    "    m = len(forget_dataset)\n",
    "    rand_idxs = idxs[:m]\n",
    "    logits_test, loss_test, test_labels = compute_attack_components_sisa1(models, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components_sisa2(models, forget_loaders)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/SISA/credit/mlp_shards={}_fr={}_base.csv'.format(num_shards, forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Train AUC', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test AUC', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n",
    "\n",
    "del models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrain \n",
    "retain_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    models = []\n",
    "    t0 = time.time()\n",
    "    for i in range(num_shards):\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        model = train_model(model, retain_loaders[i], test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "        models.append(model)\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    # Accuracy\n",
    "    retain_acc = auc_score_with_majority_voting(models, retain_loader)\n",
    "    test_acc = auc_score_with_majority_voting(models, test_loader)\n",
    "    retain_accs.append(100.0*retain_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    idxs = np.arange(len(test_dataset))\n",
    "    random.shuffle(idxs)\n",
    "    m = len(forget_dataset)\n",
    "    rand_idxs = idxs[:m]\n",
    "    logits_test, loss_test, test_labels = compute_attack_components_sisa1(models, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components_sisa1(models, forget_loader)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/SISA/credit/mlp_shards={}_fr={}_retrain.csv'.format(num_shards, forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Train AUC', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test AUC', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aaa396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
