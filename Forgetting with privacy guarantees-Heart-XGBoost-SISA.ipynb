{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d3343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import Counter\n",
    "import csv\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "from utils import *\n",
    "from datasets import *\n",
    "from mdav import *\n",
    "from train import *\n",
    "from models import *\n",
    "from attacks import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c32be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "\n",
    "# Filter out ConvergenceWarning and FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Assuming y_test and y_forget are arrays of class indices\n",
    "encoder = OneHotEncoder(sparse_output=False, categories=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_everything(seed=7):\n",
    "#     np.random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# seed_everything(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba412321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get dataset\n",
    "\n",
    "df=pd.read_csv('data/heart/cardio_train.csv', sep=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "split_ratio = 0.8  # 80% for the first DataFrame, 20% for the second\n",
    "# Perform the random split\n",
    "mask = np.random.rand(len(df)) < split_ratio\n",
    "trainset = df[mask]\n",
    "testset = df[~mask]\n",
    "# Reset the index of the new DataFrames if needed\n",
    "trainset.reset_index(drop=True, inplace=True)\n",
    "testset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "X_train = trainset.iloc[:,:-1].values\n",
    "y_train = trainset.iloc[:,-1].values\n",
    "X_test = testset.iloc[:,:-1].values\n",
    "y_test = testset.iloc[:,-1].values\n",
    "\n",
    "SC = StandardScaler()\n",
    "X_train = SC.fit_transform(X_train)\n",
    "X_test = SC.transform(X_test)\n",
    "\n",
    "# Sharding data\n",
    "forget_ratio = 0.05\n",
    "# Divide X_train into 5 equal shards\n",
    "num_shards = 5\n",
    "shard_size = len(y_train) // num_shards\n",
    "X_shards = []\n",
    "y_shards = []\n",
    "retain_sets_X = []\n",
    "retain_sets_y = []\n",
    "forget_sets_X = []\n",
    "forget_sets_y = []\n",
    "\n",
    "for i in range(num_shards):\n",
    "    # Calculate indices for slicing\n",
    "    start_idx = i * shard_size\n",
    "    end_idx = start_idx + shard_size if i < num_shards - 1 else len(y_train)\n",
    "    \n",
    "    # Slice the data to create shards\n",
    "    X_shard = X_train[start_idx:end_idx]\n",
    "    y_shard = y_train[start_idx:end_idx]\n",
    "    \n",
    "    X_shards.append(X_shard)\n",
    "    y_shards.append(y_shard)\n",
    "    \n",
    "    # Shuffle indices for random sampling\n",
    "    idxs = np.arange(len(y_shard))\n",
    "    random.shuffle(idxs)\n",
    "    m = int(len(y_shard) * forget_ratio)  # 5% forget ratio\n",
    "    \n",
    "    # Split indices for retain and forget sets\n",
    "    retain_idxs = idxs[m:]\n",
    "    forget_idxs = idxs[:m]\n",
    "    \n",
    "    # Create retain and forget sets for the shard\n",
    "    X_retain = X_shard[retain_idxs]\n",
    "    y_retain = y_shard[retain_idxs]\n",
    "    X_forget = X_shard[forget_idxs]\n",
    "    y_forget = y_shard[forget_idxs]\n",
    "    \n",
    "    retain_sets_X.append(X_retain)\n",
    "    retain_sets_y.append(y_retain)\n",
    "    forget_sets_X.append(X_forget)\n",
    "    forget_sets_y.append(y_forget)\n",
    "\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))\n",
    "    \n",
    "num_features = X_train.shape[-1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_model = XGBClassifier(num_classes= num_classes, reg_lambda=5, \n",
    "                              learning_rate=0.5, max_depth=7, n_estimators=200, device = device)\n",
    "n_repeat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define and train M on D\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "y_forget_ = np.concatenate(forget_sets_y)\n",
    "for r in range(n_repeat):\n",
    "    models = []\n",
    "    t0 = time.time()\n",
    "    for i in range(num_shards):\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        model.fit(X_shards[i], y_shards[i])\n",
    "        models.append(model)\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "\n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    # Accuracy\n",
    "    all_train_preds = []\n",
    "    all_test_preds = []\n",
    "    for model in models:\n",
    "        all_train_preds.append(model.predict(X_train))\n",
    "        all_test_preds.append(model.predict(X_test))\n",
    "    \n",
    "    all_train_preds = np.array(all_train_preds)\n",
    "    all_test_preds = np.array(all_test_preds)\n",
    "    preds_train, _ = stats.mode(all_train_preds, axis = 0, keepdims = True)\n",
    "    preds_test, _ = stats.mode(all_test_preds, axis = 0, keepdims = True)\n",
    "    train_acc = metrics.accuracy_score(y_train, preds_train.squeeze())\n",
    "    test_acc = metrics.accuracy_score(y_test, preds_test.squeeze())\n",
    "    train_accs.append(100.0*train_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    \n",
    "    #MIA\n",
    "    all_test_preds = []\n",
    "    all_forget_preds = []\n",
    "    for i, model in enumerate(models):\n",
    "        all_test_preds.append(model.predict_proba(X_test))\n",
    "        all_forget_preds.append(model.predict_proba(forget_sets_X[i]))\n",
    "        \n",
    "    all_test_preds = np.array(all_test_preds)\n",
    "    test_preds = np.mean(all_test_preds, axis=0)\n",
    "    forget_preds = np.concatenate(all_forget_preds, axis=0)\n",
    "    \n",
    "    # Convert class indices to one-hot encoding\n",
    "    y_test_one_hot = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "    y_forget_one_hot = encoder.transform(y_forget_.reshape(-1, 1))\n",
    "\n",
    "    loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "    loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget_))])\n",
    "\n",
    "    idxs = np.arange(len(y_test))\n",
    "    random.shuffle(idxs)\n",
    "    m = len(forget_preds)\n",
    "    rand_idxs = idxs[:m]\n",
    "    attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                              loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                              train_labels = y_forget_, test_labels = y_test[rand_idxs])\n",
    "\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/SISA/heart/xgb_shards={}_fr={}_base.csv'.format(num_shards, forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Train Accuracy', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test Accuracy', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78068970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define and train M on D\n",
    "retain_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "X_retain_ = np.concatenate(retain_sets_X)\n",
    "y_retain_ = np.concatenate(retain_sets_y)\n",
    "X_forget_ = np.concatenate(forget_sets_X)\n",
    "for r in range(n_repeat):\n",
    "    models = []\n",
    "    t0 = time.time()\n",
    "    for i in range(num_shards):\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        model.fit(retain_sets_X[i], retain_sets_y[i])\n",
    "        models.append(model)\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "\n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    # Accuracy\n",
    "    all_retain_preds = []\n",
    "    all_test_preds = []\n",
    "    for model in models:\n",
    "        all_retain_preds.append(model.predict(X_retain_))\n",
    "        all_test_preds.append(model.predict(X_test))\n",
    "    \n",
    "    all_retain_preds = np.array(all_retain_preds)\n",
    "    all_test_preds = np.array(all_test_preds)\n",
    "    preds_retain, _ = stats.mode(all_retain_preds, axis = 0, keepdims = True)\n",
    "    preds_test, _ = stats.mode(all_test_preds, axis = 0, keepdims = True)\n",
    "    train_acc = metrics.accuracy_score(y_retain_, preds_retain.squeeze())\n",
    "    test_acc = metrics.accuracy_score(y_test, preds_test.squeeze())\n",
    "    train_accs.append(100.0*train_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    \n",
    "    #MIA\n",
    "    all_test_preds = []\n",
    "    all_forget_preds = []\n",
    "    for model in models:\n",
    "        all_test_preds.append(model.predict_proba(X_test))\n",
    "        all_forget_preds.append(model.predict_proba(X_forget_))\n",
    "        \n",
    "    all_test_preds = np.array(all_test_preds)\n",
    "    all_forget_preds = np.array(all_forget_preds)\n",
    "    test_preds = np.mean(all_test_preds, axis=0)\n",
    "    forget_preds = np.mean(all_forget_preds, axis=0)\n",
    "\n",
    "    # Convert class indices to one-hot encoding\n",
    "    y_test_one_hot = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "    y_forget_one_hot = encoder.transform(y_forget_.reshape(-1, 1))\n",
    "\n",
    "    loss_test = np.array([metrics.log_loss(y_test_one_hot[i], test_preds[i]) for i in range(len(y_test))])\n",
    "    loss_forget = np.array([metrics.log_loss(y_forget_one_hot[i], forget_preds[i]) for i in range(len(y_forget_))])\n",
    "\n",
    "    attack_result = tf_attack(logits_train = forget_preds, logits_test = test_preds[rand_idxs], \n",
    "                              loss_train = loss_forget, loss_test = loss_test[rand_idxs], \n",
    "                              train_labels = y_forget_, test_labels = y_test[rand_idxs])\n",
    "\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Retraining time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/SISA/heart/xgb_shards={}_fr={}_retrain.csv'.format(num_shards, forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Retraining Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Retain Accuracy', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test Accuracy', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba56f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
