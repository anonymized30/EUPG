{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d3343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from datasets import *\n",
    "from mdav import *\n",
    "from train import *\n",
    "from models import *\n",
    "from attacks import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import scipy\n",
    "import csv\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee76b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "\n",
    "# Filter out ConvergenceWarning and FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_everything(seed=7):\n",
    "#     np.random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# seed_everything(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba412321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "columns = [\"age\", \"workClass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "           \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \n",
    "           \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
    "\n",
    "train_data = pd.read_csv('data/adult/adult.data', names=columns, sep=r' *, *', engine='python', na_values='?')\n",
    "test_data = pd.read_csv('data/adult/adult.test', names=columns, sep=r' *, *', skiprows=1, engine='python', na_values='?')\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"num_attr_selector\", ColumnsSelector(type='int')),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"cat_attr_selector\", ColumnsSelector(type='object')),\n",
    "    (\"cat_imputer\", CategoricalImputer(columns=['workClass','occupation', 'native-country'])),\n",
    "    (\"encoder\", CategoricalEncoder(train_data, test_data, dropFirst=True))\n",
    "])\n",
    "\n",
    "full_pipeline = FeatureUnion([(\"num_pipe\", num_pipeline), (\"cat_pipeline\", cat_pipeline)])\n",
    "\n",
    "# Drop useless columns\n",
    "train_data.drop(['fnlwgt', 'education'], axis=1, inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.drop(['fnlwgt', 'education'], axis=1, inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "# copy the data before preprocessing\n",
    "train_copy = train_data.copy()\n",
    "# convert the income column to 0 or 1 and then drop the column for the feature vectors\n",
    "train_copy[\"income\"] = train_copy[\"income\"].apply(lambda x:0 if x=='<=50K' else 1)\n",
    "# creating the feature vector \n",
    "X_train = train_copy.drop('income', axis =1)\n",
    "# target values\n",
    "y_train = train_copy['income'].values\n",
    "# pass the data through the full_pipeline\n",
    "X_train = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "# take a copy of the test data set\n",
    "test_copy = test_data.copy()\n",
    "# convert the income column to 0 or 1\n",
    "test_copy[\"income\"] = test_copy[\"income\"].apply(lambda x:0 if x=='<=50K.' else 1)\n",
    "# separating the feature vecotrs and the target values\n",
    "X_test = test_copy.drop('income', axis =1)\n",
    "y_test = test_copy['income'].values\n",
    "# preprocess the test data using the full pipeline\n",
    "# here we set the type_df param to 'test'\n",
    "X_test = full_pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "# Sharding data\n",
    "forget_ratio = 0.05\n",
    "# Divide X_train into 5 equal shards\n",
    "num_shards = 5\n",
    "shard_size = len(y_train) // num_shards\n",
    "X_shards = []\n",
    "y_shards = []\n",
    "retain_sets_X = []\n",
    "retain_sets_y = []\n",
    "forget_sets_X = []\n",
    "forget_sets_y = []\n",
    "\n",
    "for i in range(num_shards):\n",
    "    # Calculate indices for slicing\n",
    "    start_idx = i * shard_size\n",
    "    end_idx = start_idx + shard_size if i < num_shards - 1 else len(y_train)\n",
    "    \n",
    "    # Slice the data to create shards\n",
    "    X_shard = X_train[start_idx:end_idx]\n",
    "    y_shard = y_train[start_idx:end_idx]\n",
    "    \n",
    "    X_shards.append(X_shard)\n",
    "    y_shards.append(y_shard)\n",
    "    \n",
    "    # Shuffle indices for random sampling\n",
    "    idxs = np.arange(len(y_shard))\n",
    "    random.shuffle(idxs)\n",
    "    m = int(len(y_shard) * forget_ratio)  # 5% forget ratio\n",
    "    \n",
    "    # Split indices for retain and forget sets\n",
    "    retain_idxs = idxs[m:]\n",
    "    forget_idxs = idxs[:m]\n",
    "    \n",
    "    # Create retain and forget sets for the shard\n",
    "    X_retain = X_shard[retain_idxs]\n",
    "    y_retain = y_shard[retain_idxs]\n",
    "    X_forget = X_shard[forget_idxs]\n",
    "    y_forget = y_shard[forget_idxs]\n",
    "    \n",
    "    retain_sets_X.append(X_retain)\n",
    "    retain_sets_y.append(y_retain)\n",
    "    forget_sets_X.append(X_forget)\n",
    "    forget_sets_y.append(y_forget)\n",
    "\n",
    "\n",
    "# Convert each shard's train, retain and forget sets into TensorDatasets\n",
    "train_datasets = [\n",
    "    TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.int64))\n",
    "    for X, y in zip(X_shards, y_shards)\n",
    "]\n",
    "\n",
    "\n",
    "retain_datasets = [\n",
    "    TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.int64))\n",
    "    for X, y in zip(retain_sets_X, retain_sets_y)\n",
    "]\n",
    "\n",
    "forget_datasets = [\n",
    "    TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.int64))\n",
    "    for X, y in zip(forget_sets_X, forget_sets_y)\n",
    "]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Create DataLoader instances for each retain and forget dataset\n",
    "train_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in train_datasets]\n",
    "retain_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in retain_datasets]\n",
    "forget_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=False) for dataset in forget_datasets]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.int64))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.int64))\n",
    "\n",
    "retain_dataset = torch.utils.data.ConcatDataset(retain_datasets)\n",
    "forget_dataset = torch.utils.data.ConcatDataset(forget_datasets)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))\n",
    "    \n",
    "num_features = X_train.shape[-1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_model = MLPModel(num_features, 128, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-2\n",
    "n_repeat = 3\n",
    "max_epochs = 100\n",
    "patience = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define and train M on D\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    models = []\n",
    "    t0 = time.time()\n",
    "    for i in range(num_shards):\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        model = train_model(model, train_loaders[i], test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "        models.append(copy.deepcopy(model))\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    # Accuracy\n",
    "    train_acc = accuracy_with_majority_voting(models, train_loader)\n",
    "    test_acc = accuracy_with_majority_voting(models, test_loader)\n",
    "    train_accs.append(100.0*train_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    idxs = np.arange(len(test_dataset))\n",
    "    random.shuffle(idxs)\n",
    "    m = len(forget_dataset)\n",
    "    rand_idxs = idxs[:m]\n",
    "    logits_test, loss_test, test_labels = compute_attack_components_sisa1(models, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components_sisa2(models, forget_loaders)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/SISA/adult/mlp_shards={}_fr={}_base.csv'.format(num_shards, forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Train accuracy', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test accuracy', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n",
    "\n",
    "del models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrain \n",
    "retain_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    models = []\n",
    "    t0 = time.time()\n",
    "    for i in range(num_shards):\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        model = train_model(model, retain_loaders[i], test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "        models.append(copy.deepcopy(model))\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    # Accuracy\n",
    "    retain_acc = accuracy_with_majority_voting(models, retain_loader)\n",
    "    test_acc = accuracy_with_majority_voting(models, test_loader)\n",
    "    retain_accs.append(100.0*retain_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    logits_test, loss_test, test_labels = compute_attack_components_sisa1(models, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components_sisa1(models, forget_loader)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/SISA/adult/mlp_shards={}_fr={}_retrain.csv'.format(num_shards, forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Train accuracy', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test accuracy', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
